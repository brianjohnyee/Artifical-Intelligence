{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lR00DiN-lmlh"
   },
   "source": [
    "# CMPS 140 Assignment 2\n",
    "\n",
    "**Due Feb 11, 2019 11:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vze3MV4Elmli"
   },
   "source": [
    "Solve the following MDP using both value iteration and policy iteration programmatically.\n",
    "\n",
    "There is a self-driving taxi that takes from place to place. Its goal is to make the most money possible and it makes the most money in a particular town, MoneyTown. The taxi has a tendency to take routes that take it to different towns and it costs money for the taxi to drive from place to place.  \n",
    "\n",
    "There are three states that the taxi can be in: 'In MoneyTown', 'MoneyTown Suburbs', and 'Outside MoneyTown'. There are two actions that the taxi can take in each state: drive and wait. Driving costs \\\\$10. When the taxi is in money town it makes \\$30, in MoneyTown Suburbs and Outside MoneyTown it only makes $10. The reward for the taxi is:\n",
    "\n",
    "(money made - cost) \n",
    "\n",
    "For example if the taxi is driving around in MoneyTown, the reward is \\$30 - \\$10 = \\$20.\n",
    "\n",
    "If the taxi is in MoneyTown and drives, then it is still MoneyTown in the next period with probability .9, and in the MoneyTown Suburbs in the next period with probability .1. If it is MoneyTown and does not drive, these probabilities are .7 (in the MoneyTown) and .3 (in the MoneyTown Suburbs), respectively. If it is in the MoneyTown Suburbs and drives, then with probability .3 it is in MoneyTown in the next period, with probability .6 it is still in MoneyTown Suburbs in the next period, and with probability .1 it is in Outside MoneyTown in the next period. If it is in MoneyTown Suburbs and does not drive, then with probability 1 it is Outside MoneyTown next period. Finally, if it is in Outside MoneyTown and drives, then in the next period it is in MoneyTown with probability .6, and at the OutSide MoneyTown with probability .4. If it does not drive, then with probability 1 it is at Outside MoneyTown in the next period. \n",
    "\n",
    "1. Draw the MDP graphically\n",
    "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
    "  - When you're done you can embed it in the jupyter notebook using markdown syntax\n",
    "  - \\!\\[alt-text\\]\\(url\\)\n",
    "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
    "  \n",
    "2. Using a discount factor of .8, solve the MDP using value iteration (until the values have become reasonably stable). You should start with the values set to zero. You should show both the optimal policy and the optimal values.\n",
    "3. Using a discount factor of .8, solve the MDP using policy iteration (until you have complete convergence). You should start with the policy that never drives. Again, you should show both the optimal policy and the optimal values (and of course they should be the same as in 2...).\n",
    "4. Change the MDP in three different ways: by changing the discount factor, changing the transition probabilities for a single action from a single state, and by changing a reward for a single action at a single state. Each of these changes should be performed separately starting at the original MDP, resulting in three new MDPs (which you do not have to draw), each of which is different from the original MDP in a single way. In each case, the change should be so that the optimal policy changes, and you should state what the optimal policy becomes and give a short intuitive argument for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dl-4oDhc2Aam"
   },
   "source": [
    "[PLESE PUT YOUR HTML TAG FOR MDP GRAPH HERE]\n",
    "\n",
    "![alt-text](https://docs.google.com/drawings/d/e/2PACX-1vSabNwqZQ0vRZ-H0cvbpskBNwpUtiiXww80O5CFssZ12e5OO47afKRQCCJBOf_a5CSv70zncHZEFceD/pub?w=960&amp;h=720)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "08ieiqMFlmlk",
    "outputId": "5b43fc53-5afd-4ee2-a56d-f9e794c3d3d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x10ca6bea0>, {'M': defaultdict(<class 'dict'>, {'drive': {'M': 0.9, 'S': 0.1, 'O': 0}, 'stop': {'M': 0.7, 'S': 0.3, 'O': 0}}), 'S': defaultdict(<class 'dict'>, {'drive': {'M': 0.3, 'S': 0.6, 'O': 0.1}, 'stop': {'M': 0, 'S': 0, 'O': 1}}), 'O': defaultdict(<class 'dict'>, {'drive': {'M': 0.6, 'S': 0, 'O': 0.4}, 'stop': {'M': 0, 'S': 0, 'O': 1}})})\n",
      "defaultdict(<function <lambda> at 0x10ca8a158>, {'M': defaultdict(<class 'dict'>, {'drive': {'M': 20, 'S': 0, 'O': 0}, 'stop': {'M': 30, 'S': 10, 'O': 10}}), 'S': defaultdict(<class 'dict'>, {'drive': {'M': 20, 'S': 0, 'O': 0}, 'stop': {'M': 30, 'S': 10, 'O': 10}}), 'O': defaultdict(<class 'dict'>, {'drive': {'M': 20, 'S': 0, 'O': 0}, 'stop': {'M': 30, 'S': 10, 'O': 10}})})\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "import collections\n",
    "\n",
    "states=[\"M\", \"S\", \"O\"]\n",
    "actions=[\"drive\", \"stop\"]\n",
    "\n",
    "Transition_table = collections.defaultdict(lambda: collections.defaultdict(dict))\n",
    "Transition_table[\"M\"][\"drive\"][\"M\"] = 0.9\n",
    "Transition_table[\"M\"][\"drive\"][\"S\"] = 0.1\n",
    "Transition_table[\"M\"][\"drive\"][\"O\"] = 0\n",
    "Transition_table[\"M\"][\"stop\"][\"M\"] = 0.7\n",
    "Transition_table[\"M\"][\"stop\"][\"S\"] = 0.3\n",
    "Transition_table[\"M\"][\"stop\"][\"O\"] = 0\n",
    "\n",
    "Transition_table[\"S\"][\"drive\"][\"M\"] = 0.3\n",
    "Transition_table[\"S\"][\"drive\"][\"S\"] = 0.6\n",
    "Transition_table[\"S\"][\"drive\"][\"O\"] = 0.1\n",
    "Transition_table[\"S\"][\"stop\"][\"M\"] = 0\n",
    "Transition_table[\"S\"][\"stop\"][\"S\"] = 0\n",
    "Transition_table[\"S\"][\"stop\"][\"O\"] = 1\n",
    "\n",
    "\n",
    "Transition_table[\"O\"][\"drive\"][\"M\"] = 0.6\n",
    "Transition_table[\"O\"][\"drive\"][\"S\"] = 0\n",
    "Transition_table[\"O\"][\"drive\"][\"O\"] = 0.4\n",
    "Transition_table[\"O\"][\"stop\"][\"M\"] = 0\n",
    "Transition_table[\"O\"][\"stop\"][\"S\"] = 0\n",
    "Transition_table[\"O\"][\"stop\"][\"O\"] = 1\n",
    "\n",
    "print(Transition_table)\n",
    "\n",
    "Reward_table = collections.defaultdict(lambda: collections.defaultdict(dict))\n",
    "Reward_table[\"M\"][\"drive\"][\"M\"] = -10 + 30\n",
    "Reward_table[\"M\"][\"drive\"][\"S\"] = -10 + 10\n",
    "Reward_table[\"M\"][\"drive\"][\"O\"] = -10 + 10\n",
    "Reward_table[\"M\"][\"stop\"][\"M\"] = 30\n",
    "Reward_table[\"M\"][\"stop\"][\"S\"] = 10\n",
    "Reward_table[\"M\"][\"stop\"][\"O\"] = 10\n",
    "\n",
    "Reward_table[\"S\"][\"drive\"][\"M\"] = -10 + 30\n",
    "Reward_table[\"S\"][\"drive\"][\"S\"] = -10 + 10\n",
    "Reward_table[\"S\"][\"drive\"][\"O\"] = -10 + 10\n",
    "Reward_table[\"S\"][\"stop\"][\"M\"] = 30\n",
    "Reward_table[\"S\"][\"stop\"][\"S\"] = 10\n",
    "Reward_table[\"S\"][\"stop\"][\"O\"] = 10\n",
    "\n",
    "Reward_table[\"O\"][\"drive\"][\"M\"] = -10 + 30\n",
    "Reward_table[\"O\"][\"drive\"][\"S\"] = -10 + 10\n",
    "Reward_table[\"O\"][\"drive\"][\"O\"] = -10 + 10\n",
    "Reward_table[\"O\"][\"stop\"][\"M\"] = 30\n",
    "Reward_table[\"O\"][\"stop\"][\"S\"] = 10\n",
    "Reward_table[\"O\"][\"stop\"][\"O\"] = 10\n",
    "\n",
    "print(Reward_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYp67r5Zlmlj"
   },
   "source": [
    "** Please put your code below to solve the problem programmatically, You can add cells as you feel the need.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6Ju-DsOz3TF"
   },
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def value_iteration(discount, states,  actions, Transition_table, Reward_table, max_iteration=10000):\n",
    "  \n",
    "  # You might use a two dimensional array for Q table instead of dictionary.\n",
    "  #Q_table = np.zeros((len(states), len(actions)), dtype='int32')\n",
    "  # Use a dictionary to define Q_table, then you may have something like, Q_table[\"M\"][\"drive\"] = 0.0\n",
    "  Q_table = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "  # Example value in P_star: P_star[\"M\"] = \"drive\"\n",
    "  P_star = {}\n",
    "  # # Initialize V* with zeros\n",
    "  V_star = {}\n",
    "  V_star[\"M\"] = 0\n",
    "  V_star[\"S\"] = 0\n",
    "  V_star[\"O\"] = 0\n",
    "  \n",
    "  for i in range(max_iteration):\n",
    "        for s in states:\n",
    "            action1 = 0\n",
    "            action2 = 0\n",
    "            for a1 in states:\n",
    "                action1 += (Transition_table[s][actions[0]][a1] * (Reward_table[s][actions[0]][a1]+ discount*V_star[a1]))\n",
    "            for a2 in states:\n",
    "                action2 += (Transition_table[s][actions[1]][a2]* (Reward_table[s][actions[1]][a2]+ discount*V_star[a2]))\n",
    "            V_star[s] = max(action1,action2)\n",
    "            # compare which if action1 or action2 was better. \n",
    "            # if action1 (drive) is greater, then make P_star = drive\n",
    "            # else make it stop\n",
    "            if action1 > action2:\n",
    "                P_star[s] = \"drive\"\n",
    "            else:\n",
    "                P_star[s] = \"stop\"\n",
    "  print (V_star)\n",
    "  print (P_star)\n",
    "  # TODO: Implement your code here. Do the iteration until the value is stable or it exceeds max_iteration.\n",
    "  return P_star, V_star\n",
    "\n",
    "\n",
    "def policy_iteration(discount, states,  actions, Transition_table, Reward_table, max_iteration=10000):\n",
    "  \n",
    "  # You might use a two dimensional array for Q table instead of dictionary.\n",
    "  #Q_table = np.zeros((len(states), len(actions)), dtype='int32')\n",
    "  # Use a dictionary to define Q_table, then you may have something like, Q_table[\"M\"][\"drive\"] = 0.0\n",
    "  Q_table = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "  # Initialize pi* with never drive\n",
    "  P_star = {}\n",
    "  P_star[\"M\"] = \"stop\"\n",
    "  P_star[\"S\"] = \"stop\"\n",
    "  P_star[\"O\"] = \"stop\"\n",
    "  # Example value in V_star: V_star[\"M\"] = 18\n",
    "  V_star = {}\n",
    "  V_star[\"M\"] = 0\n",
    "  V_star[\"S\"] = 0\n",
    "  V_star[\"O\"] = 0\n",
    "\n",
    "  policy_stable = False\n",
    "  \n",
    "  # I am doing 25 iterations of policy iteration\n",
    "  # Same thing as value iteration, expect we do the change the policy based on\n",
    "  # what was the max in the third for loop\n",
    "  for wuir in range(25):  \n",
    "      for i in range(max_iteration):\n",
    "            for s in states:\n",
    "                action1 = 0\n",
    "                action2 = 0\n",
    "                for a1 in states:\n",
    "                    action1 += (Transition_table[s][P_star[s]][a1] * (Reward_table[s][P_star[s]][a1]+ discount*V_star[a1]))\n",
    "                V_star[s] = action1\n",
    "      for s in states:\n",
    "            temp1 = 0\n",
    "            temp2 = 0\n",
    "            temp = [P_star[s]]\n",
    "            for a1 in states:\n",
    "                temp1 += (Transition_table[s][actions[0]][a1] * (Reward_table[s][actions[0]][a1]+ discount*V_star[a1]))\n",
    "            for a2 in states:\n",
    "                temp2 += (Transition_table[s][actions[1]][a2] * (Reward_table[s][actions[1]][a2]+ discount*V_star[a2]))\n",
    "            if temp1 > temp2:\n",
    "                P_star[s] = \"drive\"\n",
    "            else:\n",
    "                P_star[s] = \"stop\"\n",
    "            if temp != [P_star[s]]:\n",
    "                policy_stable = False\n",
    "            if temp == [P_star[s]]:\n",
    "                policy_stable = True\n",
    "\n",
    "  print(V_star)\n",
    "  # TODO: Implement your code here. Do the iteration until the value is stable or it exceeds max_iteration.\n",
    "  return P_star, V_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "WTh7BNK-0D0f",
    "outputId": "08570aa5-3182-4246-86bb-2aa8be4a0842",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 97.8361669242658, 'S': 79.36630602782068, 'O': 86.70788253477585}\n",
      "{'M': 'stop', 'S': 'stop', 'O': 'drive'}\n",
      "P_star_vi {'M': 'stop', 'S': 'stop', 'O': 'drive'}\n",
      "V_star_vi {'M': 97.8361669242658, 'S': 79.36630602782068, 'O': 86.70788253477585}\n",
      "{'M': 97.8361669242658, 'S': 79.36630602782068, 'O': 86.70788253477585}\n",
      "P_star_pi {'M': 'stop', 'S': 'stop', 'O': 'drive'}\n",
      "V_star_pi {'M': 97.8361669242658, 'S': 79.36630602782068, 'O': 86.70788253477585}\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "discount = 0.8\n",
    "\n",
    "P_star_vi, V_star_vi = value_iteration(discount, states,  actions, Transition_table, Reward_table)\n",
    "print(\"P_star_vi\", P_star_vi)\n",
    "print(\"V_star_vi\", V_star_vi)\n",
    "# TODO: add your expected correct results to expected_P_star_vi and expected_V_star_vi for test cases.\n",
    "expected_P_star_vi = {'M': 'stop', 'S': 'stop', 'O': 'drive'}\n",
    "expected_V_star_vi = {'M': 97.83616682805929, 'S': 79.36630594506894, 'O': 86.70788245549603}\n",
    "assert(P_star_vi == expected_P_star_vi)\n",
    "assert(math.fabs(V_star_vi[\"M\"] - expected_V_star_vi[\"M\"]) < 0.0000001) \n",
    "assert(math.fabs(V_star_vi[\"S\"] - expected_V_star_vi[\"S\"]) < 0.0000001) \n",
    "assert(math.fabs(V_star_vi[\"O\"] - expected_V_star_vi[\"O\"]) < 0.0000001) \n",
    "\n",
    "P_star_pi, V_star_pi = policy_iteration(discount, states,  actions, Transition_table, Reward_table)\n",
    "print(\"P_star_pi\", P_star_pi)\n",
    "print(\"V_star_pi\", V_star_pi)\n",
    "# TODO: add your expected correct results to expected_P_star_pi, expected_V_star_pi for test cases.\n",
    "expected_P_star_pi = {'M': 'stop', 'S': 'stop', 'O': 'drive'}\n",
    "expected_V_star_pi = {'M': 97.83616682097912, 'S': 79.36630593897894, 'O': 86.70788244966155}\n",
    "assert(math.fabs(V_star_pi[\"M\"] - expected_V_star_pi[\"M\"]) < 0.000001) \n",
    "assert(math.fabs(V_star_pi[\"S\"] - expected_V_star_pi[\"S\"]) < 0.0000001) \n",
    "assert(math.fabs(V_star_pi[\"O\"] - expected_V_star_pi[\"O\"]) < 0.0000001) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4kAmA6M1vJ9"
   },
   "source": [
    "[YOUR ANSWER FOR 4 HERE]\n",
    "\n",
    "Case 1: changing the MDP by the discount factor;\n",
    "\n",
    "Case 2: changing the MDP by the transition probabilities for a single action from a single state;\n",
    "\n",
    "Case 3: changing the MDP by a reward for a single action at a single state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 14.529914529914528, 'S': 5.5555555555555545, 'O': 5.5555555555555545}\n",
      "{'M': 'stop', 'S': 'stop', 'O': 'stop'}\n",
      "P_star_vi {'M': 'stop', 'S': 'stop', 'O': 'stop'}\n",
      "V_star_vi {'M': 14.529914529914528, 'S': 5.5555555555555545, 'O': 5.5555555555555545}\n"
     ]
    }
   ],
   "source": [
    "#case 1 changing MDP by the discount factor\n",
    "# changed the discount factor to be out of the range (0,1)\n",
    "# made it -.8\n",
    "\n",
    "# Testing\n",
    "discount = -.8\n",
    "P_star_vi, V_star_vi = value_iteration(discount, states,  actions, Transition_table, Reward_table)\n",
    "print(\"P_star_vi\", P_star_vi)\n",
    "print(\"V_star_vi\", V_star_vi)\n",
    "\n",
    "# EXPLANATION\n",
    "# The optimal policy for state O changes from drive to stop if you make the discount\n",
    "# a negative number. This was the only way I could make the policy change. Because when changing\n",
    "# the values from the range of (0,1), the policy stated consistant. My reasoning for why when changing the \n",
    "# discount to a negative number changes the policy of state O is because the value for V_star\n",
    "# for drive was very high, but because we were multiplying by a negative number, you are essentially\n",
    "# subtracting a higher number the more optimal the policy from the equation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 90.44776119402982, 'S': 65.82089552238803, 'O': 81.49253731343282}\n",
      "{'M': 'stop', 'S': 'drive', 'O': 'drive'}\n",
      "P_star_vi {'M': 'stop', 'S': 'drive', 'O': 'drive'}\n",
      "V_star_vi {'M': 90.44776119402982, 'S': 65.82089552238803, 'O': 81.49253731343282}\n"
     ]
    }
   ],
   "source": [
    "#case 2 changing the MDP by the transition probabilities for a single action from a sginle state\n",
    "\n",
    "# transition probability being changed for S,stop,S to 1\n",
    "# I am also changing the probability of S,s,O to 0 \n",
    "# to make the stop action for state S to add up to 1\n",
    "Transition_table[\"S\"][\"stop\"][\"S\"] = 1\n",
    "Transition_table[\"S\"][\"stop\"][\"O\"] = 0\n",
    "discount = .8\n",
    "\n",
    "# printing P_star and V_star\n",
    "P_star_vi, V_star_vi = value_iteration(discount, states,  actions, Transition_table, Reward_table)\n",
    "print(\"P_star_vi\", P_star_vi)\n",
    "print(\"V_star_vi\", V_star_vi)\n",
    "\n",
    "# EXPLANATION\n",
    "# By changing the transition table probability for state S and action \"stop\"\n",
    "# the optimal policy for state S changes from \"stop\" to \"drive\".\n",
    "# This happens because, now if you stop in state O, and when in state O,\n",
    "# had more chance to drive to state M, which makes the most money.\n",
    "# Therefore, that is why the policy for state S changes from stop to drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 213.76119402985066, 'S': 291.8955223880596, 'O': 168.5373134328358}\n",
      "{'M': 'stop', 'S': 'drive', 'O': 'drive'}\n",
      "P_star_vi {'M': 'stop', 'S': 'drive', 'O': 'drive'}\n",
      "V_star_vi {'M': 213.76119402985066, 'S': 291.8955223880596, 'O': 168.5373134328358}\n"
     ]
    }
   ],
   "source": [
    "#case 3 changing the MDP by a reward for a single action at a single state\n",
    "\n",
    "# this is to reset back to original transition table from case 2\n",
    "Transition_table[\"S\"][\"stop\"][\"S\"] = 0\n",
    "Transition_table[\"S\"][\"stop\"][\"O\"] = 1\n",
    "discount = .8\n",
    "\n",
    "# reward for a single action being changed\n",
    "Reward_table[\"S\"][\"drive\"][\"M\"] = -10 + 300\n",
    "\n",
    "# printing out P_star and V_star\n",
    "P_star_vi, V_star_vi = value_iteration(discount, states,  actions, Transition_table, Reward_table)\n",
    "print(\"P_star_vi\", P_star_vi)\n",
    "print(\"V_star_vi\", V_star_vi)\n",
    "\n",
    "# EXPLANATION\n",
    "# The reward in the Reward_table I changed was S,drive,M = 290.\n",
    "# By changing the Reward for a single action at a single state the optimal policy changesfrom \n",
    "# M:stop, S:stop, O:drive, to M:stop, S:drive, O:drive\n",
    "# The optimal poilcy for S changes from stop to drive just from this one change because the reward\n",
    "# is so high, that while calcuating the max V_star between actions, the reward from driving when in\n",
    "# State S to M is much greater than the reward for stopping in state S.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment2_programming.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
